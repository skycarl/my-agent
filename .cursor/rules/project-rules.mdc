---
alwaysApply: true
---
# Configuration Management Rules

## Settings Pattern

When implementing new features that require configuration, follow this established pattern:

### 1. Configuration Class Structure

Use Pydantic Settings with the following structure in `app/core/settings.py`:

```python
from pydantic import Field
from pydantic_settings import BaseSettings, SettingsConfigDict

class Config(BaseSettings):
    """Application config."""
    
    # Group settings with clear comments
    # Feature Name Configuration
    feature_setting: str = Field(default="default_value", description="Description of what this setting does")
    feature_enabled: bool = Field(default=True, description="Enable/disable feature")
    
    model_config = SettingsConfigDict(
        env_file=".env",
        env_file_encoding="utf-8",
        case_sensitive=False,
        extra="ignore",
    )
```

### 2. Environment Variable Naming

- Use UPPER_SNAKE_CASE for environment variables
- Match the Field name converted to uppercase with underscores
- Example: `feature_setting` → `FEATURE_SETTING`

### 3. Default Values and Descriptions

- Always provide meaningful default values
- Include descriptive docstrings for each Field
- Use appropriate types (str, int, bool, etc.)
- For sensitive data (API keys, tokens), use empty string as default

### 4. Global Config Instance

Use the singleton pattern for config access:

```python
# Global config instance
_config = None

def get_config() -> Config:
    """Get the global config instance, creating it if it doesn't exist."""
    global _config
    if _config is None:
        _config = Config()
    return _config

# Create the global config instance
config = get_config()
```

### 5. Usage in Code

Import and use config in your modules:

```python
from app.core.settings import config

# Access settings
api_key = config.feature_setting
is_enabled = config.feature_enabled
```

### 6. Environment File Updates

When adding new settings:

1. Add the setting to `Config` class in `app/core/settings.py`
2. Add corresponding environment variable to `env.example` with:
   - Clear comment explaining the setting
   - Default value
   - Usage instructions if needed
3. Document any special requirements or dependencies

### 7. Testing Configuration

Use the `create_test_config` method for testing:

```python
from app.core.settings import Config

# Create test config without loading from .env
test_config = Config.create_test_config(
    feature_setting="test_value",
    feature_enabled=False
)
```

### 8. Validation and Error Handling

- Validate configuration in your feature initialization
- Provide clear error messages when required settings are missing
- Use the `is_configured()` pattern for optional features

### 9. Documentation

- Update `env.example` with all new environment variables
- Include clear comments explaining each setting
- Document default values and expected formats
- Add usage examples for complex configurations

### 10. Security Considerations

- Never commit actual API keys or tokens to version control
- Use empty string defaults for sensitive settings
- Validate and sanitize configuration values where appropriate
- Use environment-specific configuration for different deployment environments

# OpenAI API Usage Rules

## Always Use Responses API

When integrating with OpenAI's language models, ALWAYS use the **Responses API**, not the older `chat.completions` API. The Responses API provides better structured outputs and more reliable parsing.

### Always Use Manual Conversation State

ALWAYS use manual conversation state management with `store=False` when using the Responses API. Do not rely on OpenAI's automatic conversation storage.

### Basic Responses API Usage

```python
from openai import OpenAI
from app.core.settings import config

client = OpenAI(api_key=config.openai_api_key)

# Basic single request
response = client.responses.create(
    model="gpt-4.1",
    input="Write a one-sentence bedtime story about a unicorn.",
    store=False
)

print(response.output_text)
```

### Manual Conversation State Management

```python
from openai import OpenAI
from app.core.settings import config

client = OpenAI(api_key=config.openai_api_key)

# Initialize conversation history
history = [
    {"role": "user", "content": "Tell me a joke"}
]

# First request
response = client.responses.create(
    model="gpt-4o-mini",
    input=history,
    store=False
)

print(response.output_text)

# Add the response to conversation history
history += [{"role": el.role, "content": el.content} for el in response.output]

# Continue the conversation
history.append({"role": "user", "content": "Tell me another"})

second_response = client.responses.create(
    model="gpt-4o-mini",
    input=history,
    store=False
)

print(second_response.output_text)
```

### Using Tools with Responses API

```python
from openai import OpenAI
from app.core.settings import config

client = OpenAI(api_key=config.openai_api_key)

response = client.responses.create(
    model="gpt-4.1",
    tools=[{"type": "web_search_preview"}],
    input="What was a positive news story from today?",
    store=False
)

print(response.output_text)
```

### Error Handling with Responses API

```python
from openai import OpenAI
from app.core.settings import config

def query_openai(input_content: str, conversation_history: list = None):
    """Query OpenAI with proper error handling."""
    client = OpenAI(api_key=config.openai_api_key)
    
    # Use conversation history if provided, otherwise single input
    input_data = conversation_history if conversation_history else input_content
    
    try:
        response = client.responses.create(
            model="gpt-4o-mini",
            input=input_data,
            store=False
        )
        
        if response.output_text:
            return response.output_text, response.output
        else:
            logger.warning("No output text received from OpenAI")
            return None, None
            
    except Exception as e:
        logger.error(f"OpenAI Responses API error: {e}")
        raise
```

### Migration Note

If you encounter legacy code using `client.chat.completions.create()`, update it to use `client.responses.create()` with:
- `input` parameter instead of `messages`
- `store=False` for manual conversation state
- Access response via `response.output_text` or `response.output`

# OpenAI Agents SDK Usage

## Overview

Use the OpenAI Agents SDK to build agentic AI applications capable of taking action on behalf of users. The SDK provides a lightweight, production-ready framework with three core primitives:

- **Agents**: LLMs equipped with instructions and tools
- **Handoffs**: Allow agents to delegate to other agents for specific tasks  
- **Guardrails**: Enable input validation and checks

## Basic Agent Creation

Create agents with instructions, names, and optional configuration:

```python
from agents import Agent

agent = Agent(
    name="Math Tutor",
    instructions="You provide help with math problems. Explain your reasoning at each step and include examples",
    model="gpt-4o-mini"
)
```

## Agent Handoffs

Use handoffs to create specialized agents that can delegate tasks to each other:

```python
from agents import Agent, Runner
import asyncio

spanish_agent = Agent(
    name="Spanish agent",
    instructions="You only speak Spanish.",
)

english_agent = Agent(
    name="English agent", 
    instructions="You only speak English",
)

triage_agent = Agent(
    name="Triage agent",
    instructions="Handoff to the appropriate agent based on the language of the request.",
    handoffs=[spanish_agent, english_agent],
)
```

## Running Agent Workflows

Use the `Runner` to execute agent workflows:

```python
async def main():
    result = await Runner.run(triage_agent, input="Hola, ¿cómo estás?")
    print(result.final_output)

if __name__ == "__main__":
    asyncio.run(main())
```

## Agent Tools

Convert Python functions into agent tools with automatic schema generation:

```python
from agents import Agent, function_tool

@function_tool
def get_weather(city: str) -> str:
    """Get weather information for a city."""
    return f"The weather in {city} is sunny"

weather_agent = Agent(
    name="Weather Assistant",
    instructions="Help users get weather information",
    tools=[get_weather],
)
```

## Configuration Integration

Integrate agents with your project's configuration system:

```python
from app.core.settings import config
from agents import Agent

agent = Agent(
    name="Assistant",
    instructions="You are a helpful assistant",
    model=config.openai_model,  # Configure model via settings
)
```

## Best Practices

- Use descriptive agent names and clear instructions
- Leverage handoffs for complex multi-step workflows
- Keep agent responsibilities focused and specific
- Use function tools to extend agent capabilities
- Test agent workflows with various input scenarios

## Quality Assurance Requirements

### Feature Completion Checklist

Before considering any feature complete, ensure the following quality checks pass:

#### 1. Unit Tests
- Run the test suite using `make test`
- All tests must pass without errors or failures
- If new functionality is added, include corresponding unit tests
- Tests should cover both happy path and edge cases

#### 2. Code Quality
- Run linting using `make lint`
- All linting checks must pass without warnings or errors
- Fix any code style issues identified by the linter
- Ensure code follows the project's style guidelines

#### 3. Pre-Commit Validation
Before committing any feature:
1. Run `make test` to verify all tests pass
2. Run `make lint` to ensure code quality standards are met
3. Only proceed with the commit if both checks pass
4. If either check fails, fix the issues and re-run the checks

#### 4. Continuous Integration
- These same checks are enforced in CI/CD pipelines
- Features should not be merged if tests or linting fail
- Address any CI failures before requesting code review

### Example Workflow

```bash
# After implementing a feature
make test    # Verify all tests pass
make lint    # Verify code quality standards

# If both pass, proceed with commit
# If either fails, fix issues and re-run checks
```

This ensures that all code meets quality standards and maintains the project's reliability and maintainability.

## Example Implementation

When adding a new feature that requires configuration:

1. **Add to settings.py:**
```python
# New Feature Configuration
new_feature_api_key: str = Field(default="", description="API key for new feature")
new_feature_url: str = Field(default="https://api.example.com", description="Base URL for new feature API")
new_feature_timeout: int = Field(default=30, description="Timeout in seconds for API calls")
```

2. **Add to env.example:**
```bash
# New Feature Configuration
# API key for the new feature (get from provider dashboard)
NEW_FEATURE_API_KEY=
# Base URL for the new feature API (defaults to https://api.example.com)
NEW_FEATURE_URL=https://api.example.com
# Timeout in seconds for API calls (defaults to 30)
NEW_FEATURE_TIMEOUT=30
```

3. **Use in your feature:**
```python
from app.core.settings import config

class NewFeature:
    def __init__(self):
        self.api_key = config.new_feature_api_key
        self.base_url = config.new_feature_url
        self.timeout = config.new_feature_timeout
        
        if not self.api_key:
            raise ValueError("NEW_FEATURE_API_KEY is required")
```

4. **Add tests for your feature:**
Use pytest for unit tests. Add the tests to the `app/tests` directory. If you need to use a temporary directory, use the built in pytest temporary directory fixtures (e.g., `tmp_path` or `tmp_path_factory` or `tmpdir`). If you are testing several sets of inputs and outputs against the same test machinery, consider using parameterized tests with `@pytest.mark.parametrize`. 

### Test File Organization
- Create test files in `app/tests/` directory
- Name test files with `test_` prefix (e.g., `test_new_feature.py`)
- Group related tests in the same file
- Use descriptive test function names that explain what is being tested

### Test Structure and Best Practices
```python
import pytest
from unittest.mock import Mock, patch
from app.core.settings import config

# Basic test structure
def test_feature_initialization():
    """Test that feature initializes correctly with valid config."""
    # Arrange
    expected_api_key = "test_key"
    
    # Act
    feature = NewFeature()
    
    # Assert
    assert feature.api_key == expected_api_key
    assert feature.base_url == "https://api.example.com"

def test_feature_with_missing_api_key():
    """Test that feature raises error when API key is missing."""
    # Arrange
    with patch.object(config, 'new_feature_api_key', ''):
        # Act & Assert
        with pytest.raises(ValueError, match="NEW_FEATURE_API_KEY is required"):
            NewFeature()

# Using pytest fixtures for setup/teardown
@pytest.fixture
def mock_config():
    """Fixture to provide test configuration."""
    with patch.object(config, 'new_feature_api_key', 'test_key'):
        with patch.object(config, 'new_feature_url', 'https://test.api.com'):
            yield config

def test_feature_with_fixture(mock_config):
    """Test using fixture for configuration."""
    feature = NewFeature()
    assert feature.api_key == 'test_key'
    assert feature.base_url == 'https://test.api.com'

# Parameterized tests for multiple scenarios
@pytest.mark.parametrize("api_key,url,expected_error", [
    ("", "https://api.example.com", "NEW_FEATURE_API_KEY is required"),
    ("valid_key", "", "NEW_FEATURE_URL is required"),
    ("", "", "NEW_FEATURE_API_KEY is required"),
])
def test_feature_validation_errors(api_key, url, expected_error):
    """Test various validation error scenarios."""
    with patch.object(config, 'new_feature_api_key', api_key):
        with patch.object(config, 'new_feature_url', url):
            with pytest.raises(ValueError, match=expected_error):
                NewFeature()

# Using temporary directories
def test_feature_file_operations(tmp_path):
    """Test file operations using temporary directory."""
    test_file = tmp_path / "test_config.json"
    test_file.write_text('{"key": "value"}')
    
    feature = NewFeature()
    result = feature.load_config_from_file(test_file)
    
    assert result["key"] == "value"

# Async tests (if your feature uses async)
@pytest.mark.asyncio
async def test_async_feature():
    """Test async feature functionality."""
    feature = AsyncNewFeature()
    result = await feature.async_operation()
    assert result is not None

# Mocking external dependencies
@patch('requests.get')
def test_feature_api_call(mock_get):
    """Test API calls with mocked requests."""
    mock_get.return_value.status_code = 200
    mock_get.return_value.json.return_value = {"status": "success"}
    
    feature = NewFeature()
    result = feature.make_api_call()
    
    assert result["status"] == "success"
    mock_get.assert_called_once()

# Testing configuration validation
def test_config_validation():
    """Test configuration validation logic."""
    # Test valid config
    valid_config = {
        'new_feature_api_key': 'valid_key',
        'new_feature_url': 'https://api.example.com',
        'new_feature_timeout': 30
    }
    assert is_valid_config(valid_config) is True
    
    # Test invalid config
    invalid_config = {
        'new_feature_api_key': '',
        'new_feature_url': 'https://api.example.com'
    }
    assert is_valid_config(invalid_config) is False
```

### Test Categories to Cover
- **Happy Path**: Normal operation with valid inputs
- **Error Handling**: Invalid inputs, missing configuration, network errors
- **Edge Cases**: Boundary values, empty strings, None values
- **Integration**: How the feature works with other components
- **Performance**: Timeouts, large data sets (if applicable)

### Testing Guidelines
- Aim for high test coverage (80%+ for critical paths)
- Use descriptive test names that explain the scenario
- Group related tests using classes or descriptive function names
- Use fixtures for common setup and teardown
- Mock external dependencies to isolate unit tests
- Test both success and failure scenarios
- Include integration tests for complex features


5. **Run quality checks:**
```bash
make test  # Ensure all tests pass
make lint  # Ensure code quality standards are met
```

This pattern ensures consistent, maintainable, and secure configuration management across the entire application. 